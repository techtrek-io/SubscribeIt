{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caec69b3-67bd-4900-a5fb-ec9db4bd8b1b",
   "metadata": {},
   "source": [
    "# All About Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b39fe8-42b5-4528-ac91-ae0de9ff2392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install Ray libraries first!\n",
    "!pip install -U \"ray[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d470c-69e6-488e-8878-a47579dc6bb2",
   "metadata": {},
   "source": [
    "## Ray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd080f-04c8-4f36-875b-50ed41fe71fd",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912ed4a-3282-4e45-9eec-2e06bd62231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load from S3\n",
    "\n",
    "import ray\n",
    "ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\n",
    "print(ds.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cead1b6-a15c-48d8-b213-f3312a3bc595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load from HuggingFace\n",
    "\n",
    "import ray.data\n",
    "from datasets import load_dataset\n",
    "\n",
    "hf_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "ray_ds = ray.data.from_huggingface(hf_ds)\n",
    "ray_ds[\"train\"].take(2)\n",
    "\n",
    "# output: [{'text': ''}, {'text': ' = Valkyria Chronicles III = \\n'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80ca57-5fa4-442e-98b0-513d63c13894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load from MongoDB\n",
    "\n",
    "import ray\n",
    "\n",
    "# Read a local MongoDB.\n",
    "ds = ray.data.read_mongo(\n",
    "    uri=\"mongodb://localhost:27017\",\n",
    "    database=\"my_db\",\n",
    "    collection=\"my_collection\",\n",
    "    pipeline=[{\"$match\": {\"col\": {\"$gte\": 0, \"$lt\": 10}}}, {\"$sort\": \"sort_col\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bba615-2096-42a1-a503-5fdaf2b9fb39",
   "metadata": {},
   "source": [
    "### Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6881f711-9e4b-4818-b37a-dd6fb3251d67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RayTaskError(OSError)",
     "evalue": "\u001b[36mray::_get_read_tasks()\u001b[39m (pid=721, ip=10.233.126.39)\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/read_api.py\", line 1928, in _get_read_tasks\n    reader = ds.create_reader(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/datasource/image_datasource.py\", line 66, in create_reader\n    return _ImageDatasourceReader(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/datasource/image_datasource.py\", line 136, in __init__\n    super().__init__(\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/datasource/file_based_datasource.py\", line 377, in __init__\n    paths, self._filesystem = _resolve_paths_and_filesystem(paths, filesystem)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/datasource/file_based_datasource.py\", line 650, in _resolve_paths_and_filesystem\n    resolved_filesystem, resolved_path = _resolve_filesystem_and_path(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pyarrow/fs.py\", line 189, in _resolve_filesystem_and_path\n    filesystem, path = FileSystem.from_uri(path)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/_fs.pyx\", line 470, in pyarrow._fs.FileSystem.from_uri\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 115, in pyarrow.lib.check_status\nOSError: When resolving region for bucket 'ray-example-data': AWS Error NETWORK_CONNECTION during HeadBucket operation: curlCode: 28, Timeout was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(OSError)\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n\u001b[1;32m     11\u001b[0m ds \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_images\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://anonymous@ray-example-data/image-datasets/simple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(parse_filename)\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/data/read_api.py:693\u001b[0m, in \u001b[0;36mread_images\u001b[0;34m(paths, filesystem, parallelism, meta_provider, ray_remote_args, arrow_open_file_args, partition_filter, partitioning, size, mode, include_paths, ignore_missing_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;129m@PublicAPI\u001b[39m(stability\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_images\u001b[39m(\n\u001b[1;32m    602\u001b[0m     paths: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m     ignore_missing_paths: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    617\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    618\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read images from the specified paths.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m        ValueError: if ``mode`` is unsupported.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_datasource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mImageDatasource\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mray_remote_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_remote_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopen_stream_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrow_open_file_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_missing_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_missing_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/data/read_api.py:344\u001b[0m, in \u001b[0;36mread_datasource\u001b[0;34m(datasource, parallelism, ray_remote_args, **read_args)\u001b[0m\n\u001b[1;32m    331\u001b[0m     scheduling_strategy \u001b[38;5;241m=\u001b[39m NodeAffinitySchedulingStrategy(\n\u001b[1;32m    332\u001b[0m         ray\u001b[38;5;241m.\u001b[39mget_runtime_context()\u001b[38;5;241m.\u001b[39mget_node_id(),\n\u001b[1;32m    333\u001b[0m         soft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     get_read_tasks \u001b[38;5;241m=\u001b[39m cached_remote_fn(\n\u001b[1;32m    336\u001b[0m         _get_read_tasks, retry_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_cpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    337\u001b[0m     )\u001b[38;5;241m.\u001b[39moptions(scheduling_strategy\u001b[38;5;241m=\u001b[39mscheduling_strategy)\n\u001b[1;32m    339\u001b[0m     (\n\u001b[1;32m    340\u001b[0m         requested_parallelism,\n\u001b[1;32m    341\u001b[0m         min_safe_parallelism,\n\u001b[1;32m    342\u001b[0m         inmemory_size,\n\u001b[1;32m    343\u001b[0m         read_tasks,\n\u001b[0;32m--> 344\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_read_tasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdatasource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcur_pg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_wrap_arrow_serialization_workaround\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Compute the number of blocks the read will return. If the number of blocks is\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# expected to be less than the requested parallelism, boost the number of blocks\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# by adding an additional split into `k` pieces to each read task.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_tasks:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py:2524\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2522\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(OSError)\u001b[0m: \u001b[36mray::_get_read_tasks()\u001b[39m (pid=721, ip=10.233.126.39)\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/read_api.py\", line 1928, in _get_read_tasks\n    reader = ds.create_reader(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/datasource/image_datasource.py\", line 66, in create_reader\n    return _ImageDatasourceReader(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/datasource/image_datasource.py\", line 136, in __init__\n    super().__init__(\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/datasource/file_based_datasource.py\", line 377, in __init__\n    paths, self._filesystem = _resolve_paths_and_filesystem(paths, filesystem)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/ray/data/datasource/file_based_datasource.py\", line 650, in _resolve_paths_and_filesystem\n    resolved_filesystem, resolved_path = _resolve_filesystem_and_path(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pyarrow/fs.py\", line 189, in _resolve_filesystem_and_path\n    filesystem, path = FileSystem.from_uri(path)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/_fs.pyx\", line 470, in pyarrow._fs.FileSystem.from_uri\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 115, in pyarrow.lib.check_status\nOSError: When resolving region for bucket 'ray-example-data': AWS Error NETWORK_CONNECTION during HeadBucket operation: curlCode: 28, Timeout was reached"
     ]
    }
   ],
   "source": [
    "#### Transforming rows, calling map() or flat_map().\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "import ray\n",
    "\n",
    "def parse_filename(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    row[\"filename\"] = os.path.basename(row[\"path\"])\n",
    "    return row\n",
    "\n",
    "ds = (\n",
    "    ray.data.read_images(\"s3://anonymous@ray-example-data/image-datasets/simple\", include_paths=True)\n",
    "    .map(parse_filename)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41a2d7-085b-412c-8ec5-14171438d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Transforming batches, calling map_batches()\n",
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import ray\n",
    "\n",
    "class TorchPredictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = torch.nn.Identity().cuda()\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        inputs = torch.as_tensor(batch[\"data\"], dtype=torch.float32).cuda()\n",
    "        with torch.inference_mode():\n",
    "            batch[\"output\"] = self.model(inputs).detach().cpu().numpy()\n",
    "        return batch\n",
    "\n",
    "ds = (\n",
    "    ray.data.from_numpy(np.ones((32, 100)))\n",
    "    .map_batches(\n",
    "        TorchPredictor,\n",
    "        # Two workers with one GPU each\n",
    "        compute=ray.data.ActorPoolStrategy(size=2),\n",
    "        # Batch size is required if you're using GPUs.\n",
    "        batch_size=4,\n",
    "        num_gpus=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8f973-ddf6-4b0d-88bb-95a11b5e4d56",
   "metadata": {},
   "source": [
    "### Inspecting Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e451b1-db76-4f83-a503-7577c0f03075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n",
    "rows = ds.take(1)\n",
    "print(rows)\n",
    "\n",
    "# output: [{'sepal length (cm)': 5.1, 'sepal width (cm)': 3.5, 'petal length (cm)': 1.4, 'petal width (cm)': 0.2, 'target': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f6bb67-988c-4fa9-94f7-4356c7271291",
   "metadata": {},
   "source": [
    "### Iterating over Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d01cdd-a28c-4db9-8745-ab80db02cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ds = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")\n",
    "for row in ds.iter_rows():\n",
    "    print(row)\n",
    "\n",
    "# output: {'sepal length (cm)': 5.1, 'sepal width (cm)': 3.5, 'petal length (cm)': 1.4, 'petal width (cm)': 0.2, 'target': 0}\n",
    "# {'sepal length (cm)': 4.9, 'sepal width (cm)': 3.0, 'petal length (cm)': 1.4, 'petal width (cm)': 0.2, 'target': 0}\n",
    "# ...\n",
    "# {'sepal length (cm)': 5.9, 'sepal width (cm)': 3.0, 'petal length (cm)': 5.1, 'petal width (cm)': 1.8, 'target': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f0d8e-481f-49dd-9ad1-3821bb5eafff",
   "metadata": {},
   "source": [
    "### Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f1850-2ca1-4443-a3c4-3429c01d7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save to s3\n",
    "\n",
    "import ray\n",
    "ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n",
    "ds.write_parquet(\"local:///tmp/iris/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0edd59-b6d4-4c75-9f6b-000c19b33e88",
   "metadata": {},
   "source": [
    "## Ray Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d55c53d-c3a2-470c-8622-d553855ba932",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch)\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch)\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch)\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch)\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch)\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch)\n",
      "  Using cached triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.0)\n",
      "Collecting cmake (from triton==2.0.0->torch)\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/2e/51/3a4672a819b4532a378bfefad8f886cfe71057556e0d4eefb64523fd370a/cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lit (from triton==2.0.0->torch)\n",
      "  Using cached lit-16.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.27.2 lit-16.0.6 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Install torch first\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63424dc1-0ce2-4cb7-9c2d-22dbef195621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example about pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import ray\n",
    "from ray import train\n",
    "from ray.air import session, Checkpoint\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "\n",
    "# If using GPUs, set this to True.\n",
    "use_gpu = False\n",
    "\n",
    "\n",
    "input_size = 1\n",
    "layer_size = 15\n",
    "output_size = 1\n",
    "num_epochs = 3\n",
    "\n",
    "# We define a network here.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layer2(self.relu(self.layer1(input)))\n",
    "\n",
    "# We define how to train here.\n",
    "def train_loop_per_worker():\n",
    "    dataset_shard = session.get_dataset_shard(\"train\")\n",
    "    model = NeuralNetwork()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batches in dataset_shard.iter_torch_batches(\n",
    "            batch_size=32, dtypes=torch.float\n",
    "        ):\n",
    "            inputs, labels = torch.unsqueeze(batches[\"x\"], 1), batches[\"y\"]\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"epoch: {epoch}, loss: {loss.item()}\")\n",
    "\n",
    "        # Checkpointing\n",
    "        session.report(\n",
    "            {},\n",
    "            checkpoint=Checkpoint.from_dict(\n",
    "                dict(epoch=epoch, model=model.state_dict())\n",
    "            ),\n",
    "        )\n",
    "\n",
    "# Loading data\n",
    "train_dataset = ray.data.from_items([{\"x\": x, \"y\": 2 * x + 1} for x in range(200)])\n",
    "scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)\n",
    "\n",
    "# Define the trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": train_dataset},\n",
    ")\n",
    "\n",
    "# Start to train.\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3a7bf-6db6-4055-b92e-1b0bda41a4a4",
   "metadata": {},
   "source": [
    "### Distributed Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec4c91-891f-4fc5-8427-30afc5fa2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PyTorch Vs Ray\n",
    "\n",
    " import torch\n",
    " from torch.nn.parallel import DistributedDataParallel\n",
    "+from ray.air import session\n",
    "+from ray import train\n",
    "+import ray.train.torch\n",
    "\n",
    "\n",
    " def train_func():\n",
    "-    device = torch.device(f\"cuda:{session.get_local_rank()}\" if\n",
    "-        torch.cuda.is_available() else \"cpu\")\n",
    "-    torch.cuda.set_device(device)\n",
    "\n",
    "     # Create model.\n",
    "     model = NeuralNetwork()\n",
    "\n",
    "-    model = model.to(device)\n",
    "-    model = DistributedDataParallel(model,\n",
    "-        device_ids=[session.get_local_rank()] if torch.cuda.is_available() else None)\n",
    "\n",
    "+    model = train.torch.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0527bd-34f7-482a-a4e1-6b8716fcd4ad",
   "metadata": {},
   "source": [
    "### Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f96fc-4145-4780-bc6d-ada7626188e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train.torch\n",
    "from ray.air import session, Checkpoint, ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "def train_func(config):\n",
    "    n = 100\n",
    "    # create a toy dataset\n",
    "    # data   : X - dim = (n, 4)\n",
    "    # target : Y - dim = (n, 1)\n",
    "    X = torch.Tensor(np.random.normal(0, 1, size=(n, 4)))\n",
    "    Y = torch.Tensor(np.random.uniform(0, 1, size=(n, 1)))\n",
    "\n",
    "    # toy neural network : 1-layer\n",
    "    model = nn.Linear(4, 1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "    start_epoch = 0\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        # assume that we have run the session.report() example\n",
    "        # and successfully save some model weights\n",
    "        checkpoint_dict = checkpoint.to_dict()\n",
    "        model.load_state_dict(checkpoint_dict.get(\"model_weights\"))\n",
    "        start_epoch = checkpoint_dict.get(\"epoch\", -1) + 1\n",
    "\n",
    "    # wrap the model in DDP\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "        y = model.forward(X)\n",
    "        # compute loss\n",
    "        loss = criterion(y, Y)\n",
    "        # back-propagate loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        state_dict = model.state_dict()\n",
    "        checkpoint = Checkpoint.from_dict(\n",
    "            dict(epoch=epoch, model_weights=state_dict)\n",
    "        )\n",
    "        # save checkpoint in the training\n",
    "        session.report({}, checkpoint=checkpoint)\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    train_loop_config={\"num_epochs\": 2},\n",
    "    scaling_config=ScalingConfig(num_workers=2),\n",
    ")\n",
    "\n",
    "# checkpointing\n",
    "result = trainer.fit()\n",
    "\n",
    "# load checkpoint\n",
    "trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    train_loop_config={\"num_epochs\": 4},\n",
    "    scaling_config=ScalingConfig(num_workers=2),\n",
    "    resume_from_checkpoint=result.checkpoint,\n",
    ")\n",
    "result = trainer.fit()\n",
    "\n",
    "print(result.checkpoint.to_dict())\n",
    "# {'epoch': 3, 'model_weights': OrderedDict([('bias', tensor([0.0902])), ('weight', tensor([[-0.1549, -0.0861,  0.4353, -0.4116]]))]), '_timestamp': 1656108265, '_preprocessor': None, '_current_checkpoint_id': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a480b6c-7b86-4ebc-a5b0-99a750f9b923",
   "metadata": {},
   "source": [
    "### Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254c7515-8984-4b16-a5bd-541b4739db0e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-22 03:28:21,657\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-08-22 03:28:22,051\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBoostTrainer, XGBoostPredictor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ScalingConfig\n\u001b[1;32m      7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfrom_items([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m)])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/train/xgboost/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost_checkpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBoostCheckpoint\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost_predictor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBoostPredictor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBoostTrainer\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/train/xgboost/xgboost_checkpoint.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtempfile\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Optional\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_preprocessor_to_dir\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Checkpoint\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Predictor using the resulting model:\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "from ray.train.xgboost import XGBoostTrainer, XGBoostPredictor\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "train_dataset = ray.data.from_items([{\"x\": x, \"y\": x + 1} for x in range(32)])\n",
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"y\",\n",
    "    params={\"objective\": \"reg:squarederror\"},\n",
    "    scaling_config=ScalingConfig(num_workers=3),\n",
    "    datasets={\"train\": train_dataset},\n",
    ")\n",
    "result = trainer.fit()\n",
    "\n",
    "predictor = XGBoostPredictor.from_checkpoint(result.checkpoint)\n",
    "predictions = predictor.predict(np.expand_dims(np.arange(32, 64), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd1f77-c366-487f-a99e-690029e79234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Predictor:\n",
    "\n",
    "import pandas as pd\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "batch_predictor = BatchPredictor.from_checkpoint(result.checkpoint, XGBoostPredictor)\n",
    "predict_dataset = ray.data.from_pandas(pd.DataFrame({\"x\": np.arange(32)}))\n",
    "predictions = batch_predictor.predict(\n",
    "    data=predict_dataset,\n",
    "    batch_size=8,\n",
    "    min_scoring_workers=2,\n",
    ")\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ed3ce-b0c2-4bb1-be76-8cfd8229eea0",
   "metadata": {},
   "source": [
    "## Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7a36692-b282-472f-9883-9b9a22e4ccf3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-22 05:50:41</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:01.90        </td></tr>\n",
       "<tr><td>Memory:      </td><td>43.1/881.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/128 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:A800)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">        a</th><th style=\"text-align: right;\">         b</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>trainable_d2a74_00000</td><td>TERMINATED</td><td>10.233.126.174:10826</td><td style=\"text-align: right;\">0.953869 </td><td style=\"text-align: right;\">0.0268538 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.143476</td><td style=\"text-align: right;\">344.373 </td></tr>\n",
       "<tr><td>trainable_d2a74_00001</td><td>TERMINATED</td><td>10.233.126.174:10824</td><td style=\"text-align: right;\">0.533089 </td><td style=\"text-align: right;\">0.0548289 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.184188</td><td style=\"text-align: right;\">192.5   </td></tr>\n",
       "<tr><td>trainable_d2a74_00002</td><td>TERMINATED</td><td>10.233.126.174:10825</td><td style=\"text-align: right;\">0.892717 </td><td style=\"text-align: right;\">0.573544  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.14275 </td><td style=\"text-align: right;\">322.844 </td></tr>\n",
       "<tr><td>trainable_d2a74_00003</td><td>TERMINATED</td><td>10.233.126.174:10826</td><td style=\"text-align: right;\">0.555733 </td><td style=\"text-align: right;\">0.252978  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.171353</td><td style=\"text-align: right;\">200.873 </td></tr>\n",
       "<tr><td>trainable_d2a74_00004</td><td>TERMINATED</td><td>10.233.126.174:10827</td><td style=\"text-align: right;\">0.0684515</td><td style=\"text-align: right;\">0.579124  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.222048</td><td style=\"text-align: right;\"> 25.2901</td></tr>\n",
       "<tr><td>trainable_d2a74_00005</td><td>TERMINATED</td><td>10.233.126.174:10828</td><td style=\"text-align: right;\">0.403462 </td><td style=\"text-align: right;\">0.00194268</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.337543</td><td style=\"text-align: right;\">145.652 </td></tr>\n",
       "<tr><td>trainable_d2a74_00006</td><td>TERMINATED</td><td>10.233.126.174:10829</td><td style=\"text-align: right;\">0.581593 </td><td style=\"text-align: right;\">0.469742  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.216001</td><td style=\"text-align: right;\">210.425 </td></tr>\n",
       "<tr><td>trainable_d2a74_00007</td><td>TERMINATED</td><td>10.233.126.174:10830</td><td style=\"text-align: right;\">0.415749 </td><td style=\"text-align: right;\">0.267121  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.27906 </td><td style=\"text-align: right;\">150.352 </td></tr>\n",
       "<tr><td>trainable_d2a74_00008</td><td>TERMINATED</td><td>10.233.126.174:10829</td><td style=\"text-align: right;\">0.495647 </td><td style=\"text-align: right;\">0.733256  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.116086</td><td style=\"text-align: right;\">179.662 </td></tr>\n",
       "<tr><td>trainable_d2a74_00009</td><td>TERMINATED</td><td>10.233.126.174:10832</td><td style=\"text-align: right;\">0.244803 </td><td style=\"text-align: right;\">0.0615271 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        0.358397</td><td style=\"text-align: right;\"> 88.4355</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 05:50:41,181\tINFO tune.py:1148 -- Total run time: 1.92 seconds (1.89 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(\n",
      "  metrics={'score': 25.2901314545067, 'done': True, 'trial_id': 'd2a74_00004', 'experiment_tag': '4_a=0.0685,b=0.5791'},\n",
      "  path='/home/jovyan/ray_results/trainable_2023-08-22_05-50-39/trainable_d2a74_00004_4_a=0.0685,b=0.5791_2023-08-22_05-50-39',\n",
      "  checkpoint=None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Example a (x ** 2) + b, a and b are the hyperparameters we want to tune to minimize the objective\n",
    "\n",
    "from ray.air import session\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "def objective(x, a, b):  # Define an objective function.\n",
    "    return a * (x**2) + b\n",
    "\n",
    "\n",
    "def trainable(config):  # Pass a \"config\" dictionary into your trainable.\n",
    "\n",
    "    for x in range(20):  # \"Train\" for 20 iterations and compute intermediate scores.\n",
    "        score = objective(x, config[\"a\"], config[\"b\"])\n",
    "        session.report({\"score\": score})  # Send the score to Tune.\n",
    "\n",
    "space = {\"a\": tune.uniform(0, 1), \"b\": tune.uniform(0, 1)}\n",
    "tuner = tune.Tuner(\n",
    "    trainable, param_space=space, tune_config=tune.TuneConfig(num_samples=10, mode=\"min\", metric=\"score\",)\n",
    ")\n",
    "results = tuner.fit()\n",
    "print(results.get_best_result())  # Get best result object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c172f-5269-448d-9fc3-e3afebee27d3",
   "metadata": {},
   "source": [
    "## Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9cb27-b102-468c-8bc5-8cba2722693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers first\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd89780-27b0-433a-a0f5-34762ef77232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name: serve_quickstart.py\n",
    "from starlette.requests import Request\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 0.2, \"num_gpus\": 0})\n",
    "class Translator:\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        self.model = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "\n",
    "    def translate(self, text: str) -> str:\n",
    "        # Run inference\n",
    "        model_output = self.model(text)\n",
    "\n",
    "        # Post-process output to return only the translation text\n",
    "        translation = model_output[0][\"translation_text\"]\n",
    "\n",
    "        return translation\n",
    "\n",
    "    async def __call__(self, http_request: Request) -> str:\n",
    "        english_text: str = await http_request.json()\n",
    "        return self.translate(english_text)\n",
    "\n",
    "\n",
    "translator_app = Translator.bind()\n",
    "\n",
    "# server: serve run serve_quickstart:translator_app\n",
    "# client: python model_client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57c9d5-af95-463e-a701-aa9aff8e1395",
   "metadata": {},
   "source": [
    "## Ray RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e93903-620e-4d0d-af65-96c07c874071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    PPOConfig()\n",
    "    .environment(\"Taxi-v3\")\n",
    "    .rollouts(num_rollout_workers=2)\n",
    "    .framework(\"torch\")\n",
    "    .training(model={\"fcnet_hiddens\": [64, 64]})\n",
    "    .evaluation(evaluation_num_workers=1)\n",
    ")\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "for _ in range(5):\n",
    "    print(algo.train())  # 3. train it,\n",
    "\n",
    "algo.evaluate()  # 4. and evaluate it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8f3b1-87b3-4434-90af-126912726bda",
   "metadata": {},
   "source": [
    "## Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84658da-7eaa-43e4-8b9b-802fb6d93ffa",
   "metadata": {},
   "source": [
    "### Secnario1: Offline Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92371c0-2afb-4c93-9749-8f246bdc0067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install transformer first\n",
    "!pip install transformers\n",
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "\n",
    "# Step 1: Create a Ray Dataset from in-memory Numpy arrays.\n",
    "# You can also create a Ray Dataset from many other sources and file\n",
    "# formats.\n",
    "ds = ray.data.from_numpy(np.asarray([\"Complete this\", \"for me\"]))\n",
    "\n",
    "# Step 2: Define a Predictor class for inference.\n",
    "# Use a class to initialize the model just once in `__init__`\n",
    "# and re-use it for inference across multiple batches.\n",
    "class HuggingFacePredictor:\n",
    "    def __init__(self):\n",
    "        from transformers import pipeline\n",
    "        # Initialize a pre-trained GPT2 Huggingface pipeline.\n",
    "        self.model = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "    # Logic for inference on 1 batch of data.\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\n",
    "        # Get the predictions from the input batch.\n",
    "        predictions = self.model(list(batch[\"data\"]), max_length=20, num_return_sequences=1)\n",
    "        # `predictions` is a list of length-one lists. For example:\n",
    "        # [[{'generated_text': 'output_1'}], ..., [{'generated_text': 'output_2'}]]\n",
    "        # Modify the output to get it into the following format instead:\n",
    "        # ['output_1', 'output_2']\n",
    "        batch[\"output\"] = [sequences[0][\"generated_text\"] for sequences in predictions]\n",
    "        return batch\n",
    "\n",
    "# Use 2 parallel actors for inference. Each actor predicts on a\n",
    "# different partition of data.\n",
    "scale = ray.data.ActorPoolStrategy(size=2)\n",
    "# Step 3: Map the Predictor over the Dataset to get predictions.\n",
    "predictions = ds.map_batches(HuggingFacePredictor, compute=scale)\n",
    "# Step 4: Show one prediction output.\n",
    "predictions.show(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180fb11e-98a6-432b-b5cc-8ae25fdb2366",
   "metadata": {},
   "source": [
    "### Secnario2: Distributed Data Ingest with Ray Data and Ray Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8773a4b-6524-4ea6-8e9c-8948781e76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.air import session\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "# Load the data.\n",
    "train_ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\n",
    "## Uncomment to randomize the block order each epoch.\n",
    "# train_ds = train_ds.randomize_block_order()\n",
    "\n",
    "\n",
    "# Define a preprocessing function.\n",
    "def normalize_length(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    new_col = batch[\"sepal.length\"] / np.max(batch[\"sepal.length\"])\n",
    "    batch[\"normalized.sepal.length\"] = new_col\n",
    "    del batch[\"sepal.length\"]\n",
    "    return batch\n",
    "\n",
    "\n",
    "# Preprocess your data any way you want. This will be re-run each epoch.\n",
    "# You can use Ray Data preprocessors here as well,\n",
    "# e.g., preprocessor.fit_transform(train_ds)\n",
    "train_ds = train_ds.map_batches(normalize_length)\n",
    "\n",
    "\n",
    "def train_loop_per_worker():\n",
    "    # Get an iterator to the dataset we passed in below.\n",
    "    it = session.get_dataset_shard(\"train\")\n",
    "\n",
    "    # Train for 10 epochs over the data. We'll use a shuffle buffer size\n",
    "    # of 10k elements, and prefetch up to 10 batches of size 128 each.\n",
    "    for _ in range(10):\n",
    "        for batch in it.iter_batches(\n",
    "            local_shuffle_buffer_size=10000, batch_size=128, prefetch_batches=10\n",
    "        ):\n",
    "            print(\"Do some training on batch\", batch)\n",
    "\n",
    "\n",
    "my_trainer = TorchTrainer(\n",
    "    train_loop_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=2),\n",
    "    datasets={\"train\": train_ds},\n",
    ")\n",
    "my_trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
